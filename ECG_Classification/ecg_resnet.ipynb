{"cells":[{"cell_type":"markdown","metadata":{"id":"50Nc41dZkEHF"},"source":["# Import Libs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqWSYPn8kGKF"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32NykJ1fkJ9s"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datetime\n","\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n"]},{"cell_type":"markdown","metadata":{"id":"-X_Vy8jxkj68"},"source":["# Dataset Management functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19348,"status":"ok","timestamp":1646003015276,"user":{"displayName":"Gabriele Berrera","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01346685001118480433"},"user_tz":-60},"id":"hVBGUwnQkgrq","outputId":"eba16108-0927-4ace-ea69-21e91f277d74"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24404,"status":"ok","timestamp":1646003812813,"user":{"displayName":"Gabriele Berrera","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01346685001118480433"},"user_tz":-60},"id":"Z_VQ6YkxktTQ","outputId":"04c14b27-4420-4258-ebbf-814910b6003a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  drive/MyDrive/Tesi/ECG/dataset.zip\n","  inflating: dataset/X_test.npy      \n","  inflating: dataset/X_train.npy     \n","  inflating: dataset/y_test.csv      \n","  inflating: dataset/y_train.csv     \n"]}],"source":["!unzip drive/MyDrive/Tesi/ECG/dataset.zip -d dataset/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_aT55OpWlzLs"},"outputs":[],"source":["data_path = \"dataset/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Trl4e6ll444"},"outputs":[],"source":["def load_data(path):\n","\n","  X_train = np.load(path + \"X_train.npy\")\n","  Y_train = pd.read_csv(path + \"y_train.csv\")\n","\n","  X_test = np.load(path + \"X_test.npy\")\n","  Y_test = pd.read_csv(path + \"y_test.csv\")\n","\n","  calsses = list(Y_train.columns)\n","\n","  return X_train, Y_train.to_numpy(), X_test, Y_test.to_numpy(), calsses"]},{"cell_type":"markdown","metadata":{"id":"uCsBiKPZoQKw"},"source":["# ResNet Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70EyOqoiezx1"},"outputs":[],"source":["def get_resnet_multioutput(input_shape, classes):\n","    \n","    n_feature_maps = 32\n","\n","    input_layer = keras.layers.Input(input_shape)\n","\n","    # Block 1\n","    conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n","    conv_x = keras.layers.BatchNormalization()(conv_x)\n","    conv_x = keras.layers.Activation('relu')(conv_x)\n","\n","    conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n","    conv_y = keras.layers.BatchNormalization()(conv_y)\n","    conv_y = keras.layers.Activation('relu')(conv_y)\n","\n","    conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n","    conv_z = keras.layers.BatchNormalization()(conv_z)\n","\n","    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n","    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n","\n","    output_block_1 = keras.layers.add([shortcut_y, conv_z])\n","    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n","\n","    # Block 2\n","    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n","    conv_x = keras.layers.BatchNormalization()(conv_x)\n","    conv_x = keras.layers.Activation('relu')(conv_x)\n","\n","    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n","    conv_y = keras.layers.BatchNormalization()(conv_y)\n","    conv_y = keras.layers.Activation('relu')(conv_y)\n","\n","    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n","    conv_z = keras.layers.BatchNormalization()(conv_z)\n","\n","    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n","    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n","\n","    output_block_2 = keras.layers.add([shortcut_y, conv_z])\n","    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n","\n","    # Block 3\n","    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n","    conv_x = keras.layers.BatchNormalization()(conv_x)\n","    conv_x = keras.layers.Activation('relu')(conv_x)\n","\n","    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n","    conv_y = keras.layers.BatchNormalization()(conv_y)\n","    conv_y = keras.layers.Activation('relu')(conv_y)\n","\n","    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n","    conv_z = keras.layers.BatchNormalization()(conv_z)\n","\n","    shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n","\n","    output_block_3 = keras.layers.add([shortcut_y, conv_z])\n","    output_block_3 = keras.layers.Activation('relu')(output_block_3)\n","\n","    # Final Block\n","    gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n","    \n","    outputs = []\n","    for c in classes:\n","      outputs.append(keras.layers.Dense(1, activation='sigmoid', name=c + \"_out\")(gap_layer))\n","\n","    model = keras.models.Model(inputs=input_layer, outputs=outputs)\n","    \n","    return model"]},{"cell_type":"code","source":["def get_ECGNet(input_shape, classes):\n","\n","    input_layer = keras.layers.Input(input_shape)\n","\n","    outputs = []\n","    for c in classes:\n","      if c == \"HYP\":\n","        outputs.append(get_backbone(input_layer, c + \"_out\", 64))\n","      else:\n","        outputs.append(get_backbone(input_layer, c + \"_out\", 32))\n","\n","    model = keras.models.Model(inputs=input_layer, outputs=outputs)\n","    \n","    return model"],"metadata":{"id":"nwlOwtbKLOGJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_backbone(input, out_name, filters):\n","\n","    n_feature_maps = filters\n","\n","    # Block 1\n","    conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input)\n","    conv_x = keras.ayers.BatchNormalization()(conv_x)\n","    conv_x = keras.layers.Activation('relu')(conv_x)\n","\n","    conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n","    conv_y = keras.layers.BatchNormalization()(conv_y)\n","    conv_y = keras.layers.Activation('relu')(conv_y)\n","\n","    conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n","    conv_z = keras.layers.BatchNormalization()(conv_z)\n","\n","    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input)\n","    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n","\n","    output_block_1 = keras.layers.add([shortcut_y, conv_z])\n","    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n","\n","    # Block 2\n","    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n","    conv_x = keras.layers.BatchNormalization()(conv_x)\n","    conv_x = keras.layers.Activation('relu')(conv_x)\n","\n","    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n","    conv_y = keras.layers.BatchNormalization()(conv_y)\n","    conv_y = keras.layers.Activation('relu')(conv_y)\n","\n","    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n","    conv_z = keras.layers.BatchNormalization()(conv_z)\n","\n","    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n","    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n","\n","    output_block_2 = keras.layers.add([shortcut_y, conv_z])\n","    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n","\n","    # Block 3\n","    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n","    conv_x = keras.layers.BatchNormalization()(conv_x)\n","    conv_x = keras.layers.Activation('relu')(conv_x)\n","\n","    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n","    conv_y = keras.layers.BatchNormalization()(conv_y)\n","    conv_y = keras.layers.Activation('relu')(conv_y)\n","\n","    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n","    conv_z = keras.layers.BatchNormalization()(conv_z)\n","\n","    shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n","\n","    output_block_3 = keras.layers.add([shortcut_y, conv_z])\n","    output_block_3 = keras.layers.Activation('relu')(output_block_3)\n","\n","    # Final Block\n","    gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n","\n","    output = keras.layers.Dense(1, activation='sigmoid', name=out_name)(gap_layer)\n","    \n","    return output"],"metadata":{"id":"R2JwtPkdLk9z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hqM1KPLrUoy"},"source":["# Train and Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iaWo1gH3llR"},"outputs":[],"source":["def train(model, train_X, train_Y, val_X, val_Y, epochs = 30, batch_size = 64, log_dir = \"logs/train/\"):\n","\n","  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n","                                                        histogram_freq=1)\n","  \n","  early_stop  = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n","                                                 patience=20)\n","  \n","  check_point = tf.keras.callbacks.ModelCheckpoint('best_model', \n","                                                   monitor='val_loss', \n","                                                   save_weights_only=True, \n","                                                   save_best_only=True)\n","  \n","  history = model.fit(train_X, \n","                      train_Y, \n","                      epochs=epochs, \n","                      batch_size = batch_size, \n","                      validation_data = (val_X, val_Y),\n","                      callbacks=[tensorboard_callback, check_point],\n","                      verbose = 1)\n","  \n","  return history.history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gK0uxM3c7Xx7"},"outputs":[],"source":["def test(model, test_X, test_y, batch_size = 64, plot_features = False):\n","\n","  predictions = model.predict(test_X, batch_size=batch_size)\n","\t\n","  return predictions"]},{"cell_type":"markdown","metadata":{"id":"IM5tBQfFsbqC"},"source":["# Run Experiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8dMgFCc94nQ"},"outputs":[],"source":["%rm -rf ./logs/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3beLPomsdzE"},"outputs":[],"source":["n_test = 2\n","\n","X_train, Y_train, X_test, Y_test, classes = load_data(data_path)\n","X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n","n_timesteps, n_features = X_train.shape[1], X_train.shape[2]\n","\n","print(X_train.shape, Y_train.shape)\n","print(X_test.shape, Y_test.shape)\n","\n","# Save the best model\n","best_score = 0\n","best_model = None\n","scores = []\n","\n","# get the model and compile\n","if n_test == 1:\n","  model = get_resnet_multioutput((n_timesteps,n_features), classes)\n","else:\n","  model = get_ECGNet((n_timesteps,n_features), classes)\n","  \n","losses = {c + \"_out\": keras.losses.BinaryCrossentropy() for c in classes}\n","metrics = {c + \"_out\": [tf.keras.metrics.AUC(curve=\"PR\", name=c + \"_prauc\"), \n","                        tf.keras.metrics.AUC(name=c + \"_rocauc\"),\n","                        \"accuracy\"] for c in classes}\n","\n","model.compile(loss=losses, optimizer='adam', metrics=metrics)\n","\n","Y_train = [Y_train[:,i] for i in range(Y_train.shape[1])]\n","Y_val = [Y_val[:,i] for i in range(Y_val.shape[1])]\n","# train and test\n","history = train(model, X_train, Y_train, X_val, Y_val, epochs=20, log_dir = \"logs/\")\n","  \n","model.load_weights('best_model')\n","\n","preds = model.predict(X_test, batch_size=64)\n"]},{"cell_type":"code","source":["np.save(\"drive/MyDrive/Tesi/ECG/preds{}\".format(n_test), np.concatenate(preds, axis=1))\n","np.save(\"drive/MyDrive/Tesi/ECG/y{}\".format(n_test), Y_test)\n","\n","import pickle\n","\n","with open('drive/MyDrive/Tesi/ECG/history{}.pkl'.format(n_test), \"wb\") as f:\n","  pickle.dump(history,f)"],"metadata":{"id":"ZYWsl_m2EuxK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Plot train"],"metadata":{"id":"pncqVZJQxFUZ"}},{"cell_type":"code","source":["import pickle\n","import matplotlib.pyplot as plt\n","\n","with open('drive/MyDrive/Tesi/ECG/history{}.pkl'.format(n_test), 'rb') as f:\n","  history = pickle.load(f)\n","\n","best_epoch = np.argmin(history[\"val_loss\"])\n","val_pr_auc = 0\n","val_roc_auc = 0\n","val_acc = 0\n","\n","for c in classes:\n","  val_pr_auc += history[\"val_{}_out_{}_prauc\".format(c,c)][best_epoch]\n","  val_roc_auc += history[\"val_{}_out_{}_rocauc\".format(c,c)][best_epoch]\n","  val_acc += history[\"val_{}_out_accuracy\".format(c,c)][best_epoch]\n","\n","val_pr_auc = val_pr_auc / len(classes)\n","val_roc_auc = val_roc_auc / len(classes)\n","val_acc = val_acc / len(classes)\n","\n","print(\"ROC-AUC:\",val_roc_auc)\n","print(\"PR-AUC:\",val_pr_auc)\n","print(\"Accuracy:\",val_acc)\n","\n","SMALL_SIZE = 16\n","MEDIUM_SIZE = 20\n","BIGGER_SIZE = 24\n","\n","plt.rc('font', size=SMALL_SIZE)         \n","plt.rc('axes', titlesize=BIGGER_SIZE)   \n","plt.rc('axes', labelsize=MEDIUM_SIZE)\n","plt.rc('xtick', labelsize=SMALL_SIZE)\n","plt.rc('ytick', labelsize=SMALL_SIZE)\n","plt.rc('legend', fontsize=SMALL_SIZE)\n","plt.rc('figure', titlesize=BIGGER_SIZE)\n","\n","fig1, (ax1, ax2) = plt.subplots(1,2)\n","fig1.set_size_inches(18, 7)\n","\n","x = np.arange(20) + 1\n","\n","for c in classes:\n","  loss = history[\"val_{}_out_loss\".format(c)]\n","  auc = history[\"val_{}_out_{}_prauc\".format(c,c)]\n","  ax1.plot(x, loss, linewidth=2)\n","  ax2.plot(x, auc, linewidth=2)\n","\n","ax1.set_title(\"Validation Loss\")\n","ax2.set_title(\"Validation PR-AUC\")\n","\n","ax1.set_xlabel(\"epoch\")\n","ax1.set_ylabel(\"loss\")\n","ax2.set_xlabel(\"epoch\")\n","ax2.set_ylabel(\"AUC (%)\")\n","ax1.legend(['CD', 'HYP', 'MI', 'NORM', 'STTC'])\n","ax2.legend(['CD', 'HYP', 'MI', 'NORM', 'STTC'])\n","plt.show()"],"metadata":{"id":"b7L0iozTxFDq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Plot test"],"metadata":{"id":"BPhSG0waw-f5"}},{"cell_type":"code","source":["n_test = 2\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import auc\n","import matplotlib.pyplot as plt\n","\n","pred_test = np.load(\"drive/MyDrive/Tesi/ECG/preds{}.npy\".format(n_test))\n","Y_test = np.load(\"drive/MyDrive/Tesi/ECG/y{}.npy\".format(n_test))\n","\n","SMALL_SIZE = 16\n","MEDIUM_SIZE = 20\n","BIGGER_SIZE = 24\n","\n","plt.rc('font', size=SMALL_SIZE)         \n","plt.rc('axes', titlesize=BIGGER_SIZE)   \n","plt.rc('axes', labelsize=MEDIUM_SIZE)\n","plt.rc('xtick', labelsize=SMALL_SIZE)\n","plt.rc('ytick', labelsize=SMALL_SIZE)\n","plt.rc('legend', fontsize=SMALL_SIZE)\n","plt.rc('figure', titlesize=BIGGER_SIZE)\n","\n","fig, ax = plt.subplots()\n","fig.set_size_inches(11, 10)\n","\n","#pred_test = np.digitize(np.concatenate(preds, axis=1), [0.5])\n","\n","classes = ['CD', 'HYP', 'MI', 'NORM', 'STTC']\n","colors = ['blue', 'orange', 'green', 'red', 'purple']\n","\n","avg_prauc = 0\n","avg_rocauc = 0\n","avg_acc = 0\n","for i, (c, color) in enumerate(zip(classes, colors)):\n","    precision, recall, _ = precision_recall_curve(Y_test[:,i], pred_test[:,i])\n","    ax.plot(recall, precision, label=c, linewidth=3)\n","    avg_prauc += auc(recall, precision)\n","    avg_rocauc += roc_auc_score(Y_test[:,i], pred_test[:,i])\n","    avg_acc += accuracy_score(Y_test[:,i], (pred_test[:,i] > 0.5).astype(int))\n","\n","for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n","\tlabel.set_fontsize(14)\n","\n","ax.set_title('PR Curve', fontsize=24)\n","ax.legend(classes)\n","ax.set_xlabel(\"Recall\")\n","ax.set_ylabel(\"Precision\")\n","plt.show()\n","\n","print()\n","print(\"Avg. PR-AUC:\", avg_prauc / len(classes))\n","print(\"Avg. ROC-AUC:\", avg_rocauc / len(classes))\n","print(\"Avg. Accuracy:\", avg_acc / len(classes))\n"],"metadata":{"id":"tYnRfzrDyc4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ccIN5pJdrM7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(pred_test > 0.5).astype(int)"],"metadata":{"id":"pgwA0_xUw7fJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qLSULQkm19Wv"},"source":["# Tensor Board"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtlNFL7L18Ao"},"outputs":[],"source":["%tensorboard --logdir logs/"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ecg_resnet.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}