{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"har_cnn.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0eWrrssDt3pV"},"source":["# Import Libs\n"]},{"cell_type":"markdown","metadata":{"id":"HgYIY_ehW0C0"},"source":["Load tensorboard"]},{"cell_type":"code","metadata":{"id":"PsCiuOQT2Iew"},"source":["%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4nxUQ718W5XV"},"source":["Install Umap for plotting features"]},{"cell_type":"code","metadata":{"id":"mPKd6GLQWw46"},"source":["!pip install umap-learn[plot]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q3B1YtOAXCgS"},"source":["Import libs"]},{"cell_type":"code","metadata":{"id":"3Ec4pio-tEHz"},"source":["from numpy import mean\n","from numpy import std\n","from numpy import dstack\n","import numpy as np\n","import random\n","import cv2\n","from pandas import read_csv\n","import datetime\n","\n","from sklearn.metrics import plot_confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils import shuffle\n","\n","import matplotlib.pyplot as plt\n","import umap\n","import umap.plot\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from keras.models import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dropout\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from tensorflow.keras.utils import to_categorical"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bcNXFLMUt_KQ"},"source":["# Load Data\n","\n","Create the necessary functions to load the data"]},{"cell_type":"code","metadata":{"id":"qyk7AYgutLFO"},"source":["# load a single file as a numpy array\n","def load_file(filepath):\n","\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n","\treturn dataframe.values\n"," \n","# load a list of files and return as a 3d numpy array\n","def load_group(filenames, prefix=''):\n","\tloaded = list()\n","\tfor name in filenames:\n","\t\tdata = load_file(prefix + name)\n","\t\tloaded.append(data)\n","\t# stack group so that features are the 3rd dimension\n","\tloaded = dstack(loaded)\n","\treturn loaded\n"," \n","# load a dataset group, such as train or test\n","def load_dataset_group(group, prefix=''):\n","\tfilepath = prefix + group + '/Inertial Signals/'\n","\t# load all 9 files as a single array\n","\tfilenames = list()\n","\t# total acceleration\n","\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n","\t# body acceleration\n","\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n","\t# body gyroscope\n","\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n","\t# load input data\n","\tX = load_group(filenames, filepath)\n","\t# load class output\n","\ty = load_file(prefix + group + '/y_'+group+'.txt')\n","\treturn X, y\n"," \n","# load the dataset, returns train and test X and y elements\n","def load_dataset(prefix=''):\n","\t\n","  # load all train\n","\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n","  \n","\t# load all test\n","\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n","\n","\t# zero-offset class values\n","\ttrainy = trainy - 1\n","\ttesty = testy - 1\n","\n","\t# one hot encode y\n","\ttrainy = to_categorical(trainy)\n","\ttesty = to_categorical(testy)\n","  \n","\tprint(\"#### DATASET ####\")\n","\tprint(\" TRAIN DATA\")\n","\tprint(\" Data shape\", trainX.shape, \"Labels shape\", trainy.shape)\n","\tprint(\" TEST DATA\")\n","\tprint(\" Data shape\", testX.shape, \"Labels shape\", testy.shape)\n","\n","\treturn trainX, trainy, testX, testy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJw7tTvsunDR"},"source":["# Model\n","\n","Define a function to create the convolutional network"]},{"cell_type":"code","metadata":{"id":"AAnc57qo1SLw"},"source":["def get_HARNet(input_shape, n_outputs):\n","  \n","  model = Sequential()\n","  \n","  # First Block\n","  model.add(Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=input_shape))\n","  model.add(Dropout(0.5))\n","  model.add(MaxPooling1D(pool_size=2))\n","\n","  # Second Block\n","  model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(MaxPooling1D(pool_size=2))\n","\n","  # Fully connected layers\n","  model.add(Flatten(name=\"features\"))\n","  model.add(Dense(128, activation='relu'))\n","  model.add(Dense(n_outputs, activation='softmax'))\n"," \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1D-ResNet model"],"metadata":{"id":"-2WVYun-wpVm"}},{"cell_type":"code","source":["def conv_block(n_filters):\n","\n","    conv = Sequential()\n","\n","    conv.add(keras.layers.Conv1D(filters=n_filters, kernel_size=8, padding='same'))\n","    conv.add(keras.layers.BatchNormalization())\n","    conv.add(keras.layers.Activation('relu'))\n","\n","    conv.add(keras.layers.Conv1D(filters=n_filters, kernel_size=5, padding='same'))\n","    conv.add(keras.layers.BatchNormalization())\n","    conv.add(keras.layers.Activation('relu'))\n","\n","    conv.add(keras.layers.Conv1D(filters=n_filters, kernel_size=3, padding='same'))\n","    conv.add(keras.layers.BatchNormalization())\n","\n","    return conv"],"metadata":{"id":"TQbvDXvgRJJ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def shortcut(n_filters = None):\n","\n","    shortcut = Sequential()\n","\n","    # if n_filters is set, the shape of the input is changed according to it\n","    if n_filters:\n","      shortcut.add(keras.layers.Conv1D(filters=n_filters, kernel_size=1, padding='same'))\n","\n","    shortcut.add(keras.layers.BatchNormalization())\n","\n","    return shortcut"],"metadata":{"id":"5V8vaknSTkTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70EyOqoiezx1"},"outputs":[],"source":["def get_resnet(input_shape, n_outputs):\n","\n","    input_layer = keras.layers.Input(input_shape)\n","\n","    # Block 1\n","    conv_1 = conv_block(n_filters = 64)(input_layer)\n","    short_1 = shortcut(n_filters = 64)(input_layer)\n","\n","    output_1 = keras.layers.add([conv_1, short_1])\n","    output_1 = keras.layers.Activation('relu')(output_1)\n","\n","    # Block 2\n","    conv_2 = conv_block(n_filters = 128)(output_1)\n","    short_2 = shortcut(n_filters = 128)(output_1)\n","\n","    output_2 = keras.layers.add([conv_2, short_2])\n","    output_2 = keras.layers.Activation('relu')(output_2)\n","\n","    # Block 3\n","    conv_3 = conv_block(n_filters = 128)(output_2)\n","    short_3 = shortcut()(output_2)\n","\n","    output_3 = keras.layers.add([conv_3, short_3])\n","    output_3 = keras.layers.Activation('relu')(output_3)\n","\n","    # Final output\n","    final_output = keras.layers.GlobalAveragePooling1D(name=\"features\")(output_3)\n","    final_output = keras.layers.Dense(n_outputs, activation='softmax')(final_output)  \n","    model = keras.models.Model(inputs=input_layer, outputs=final_output)\n","    \n","    return model"]},{"cell_type":"code","source":["model = get_resnet((128, 9), 6)"],"metadata":{"id":"-Nmsan0iMg6I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.utils.plot_model(model, to_file=\"cnn.png\", show_shapes=True)"],"metadata":{"id":"PABuqNOhM2P_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Transformation"],"metadata":{"id":"1Ttq4urKHc0_"}},{"cell_type":"code","source":["def standardize_data(train_X, test_X):\n","\n","\ttmp_train_X = train_X.reshape((train_X.shape[0] * train_X.shape[1], train_X.shape[2]))\n","\ttmp_test_X = test_X.reshape((test_X.shape[0] * test_X.shape[1], test_X.shape[2]))\n"," \n","\tstd = StandardScaler()\n","\tstd.fit(tmp_train_X)\n","\n","\ttmp_train_X = std.transform(tmp_train_X)\n","\ttmp_test_X = std.transform(tmp_test_X)\n","\n","\treturn tmp_train_X.reshape((train_X.shape)), tmp_test_X.reshape((test_X.shape))"],"metadata":{"id":"dWgOMVdVHkRe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-RbsKzEysS0U"},"source":["# Train and Test\n","Define train and test functions"]},{"cell_type":"code","metadata":{"id":"9iaWo1gH3llR"},"source":["def train(model, repeat, train_X, train_y, validation_split = 0.30, epochs = 10, batch_size = 64, log_dir = \"logs/train/\"):\n","\n","  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","  early_stop  = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n","                                                 patience=10)\n","  \n","  check_point = tf.keras.callbacks.ModelCheckpoint('best_model_r{}'.format(repeat), \n","                                                   monitor='val_loss', \n","                                                   save_weights_only=True, \n","                                                   save_best_only=True)\n","  \n","  history = model.fit(train_X, \n","                      train_y, \n","                      epochs=epochs, \n","                      batch_size = batch_size, \n","                      validation_split = validation_split,\n","                      callbacks=[tensorboard_callback, check_point],\n","                      verbose = 1)\n","\n","  return history.history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gK0uxM3c7Xx7"},"source":["def test(model, test_X, test_y, batch_size = 64, plot_features = False):\n","\n","  if plot_features:\n","    XX = model.input \n","    YY = model.get_layer(name=\"features\").output\n","    feature_extractor = Model(XX, YY)\n","\n","    features = feature_extractor(test_X).numpy()\n","    features = np.reshape(features, (features.shape[0], -1))\n","\n","    labels = np.argmax(test_y, axis = 1)\n","    mapper = umap.UMAP().fit(features)\n","    umap.plot.points(mapper, labels = labels)\n","\n","  predictions = model.predict(test_X, batch_size=batch_size)\n","\t\n","  return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fqKbZ6t9rvgs"},"source":["# Environment Preparation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w-TZ3iJyvOnU","executionInfo":{"status":"ok","timestamp":1644944213624,"user_tz":-60,"elapsed":23288,"user":{"displayName":"Gabriele Berrera","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01346685001118480433"}},"outputId":"681a3380-bcec-40f6-c6ac-77b0482701f6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"DLg0K6mDvaZ6"},"source":["folder = \"drive/MyDrive/Tesi\"\n","\n","!ls drive/MyDrive/Tesi\n","!unzip drive/MyDrive/Tesi/HARDataset.zip\n","!mv UCI\\ HAR\\ Dataset HARDataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I3tSicNsrTgC"},"source":["%rm -rf ./logs/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rTRp2TKYr6RU"},"source":["# Run Experiment"]},{"cell_type":"code","source":["batch_size = 64\n","repeats = 10\n","epochs = 50"],"metadata":{"id":"Ulenz1wyZLPn"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxvSydOcHcqw"},"source":["\n","# load data\n","trainX, trainy, testX, testy = load_dataset()\n","n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n","\n","trainX, testX = standardize_data(trainX, testX)\n","\n","# Save the best model\n","histories = []\n","\n","for r in range(repeats):\n","\n","  print(\"\\nExperiment #{}\".format(r + 1))\n","\n","  # shuffle data\n","  trainX, trainy = shuffle(trainX,trainy)\n","\n","  # get the model and compile\n","  model = get_HARNet((n_timesteps,n_features), n_outputs)\n","  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  # train and test\n","  hist = train(model, r, trainX, trainy, batch_size = batch_size, \n","                                 epochs = epochs, log_dir = \"logs/train/exp{}\".format(r +1))\n","  \n","  # save history\n","  histories.append(hist)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sghA4EGKr__r"},"source":["# Show Results"]},{"cell_type":"markdown","source":["Show training results"],"metadata":{"id":"8M6v_uUcvKEu"}},{"cell_type":"code","source":["# plot losses and accuracies over epochs\n","\n","fig1, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n","fig1.subplots_adjust(hspace=0.6, wspace=0.4)\n","fig1.set_size_inches(14, 9)\n","fig1.suptitle(\"CNN Training\", fontsize=20)\n","\n","val_best_losses =[]\n","val_best_accuracies =[]\n","\n","total_losses = [0] * epochs\n","total_val_losses = [0] * epochs\n","total_accs = [0] * epochs\n","total_val_accs = [0] * epochs\n","\n","x = np.arange(epochs) + 1\n","\n","for h in histories:\n","\n","  losses = h[\"loss\"]\n","  val_losses = h[\"val_loss\"]\n","  accs = h[\"accuracy\"]\n","  val_accs = h[\"val_accuracy\"]\n","\n","  total_losses = np.add(total_losses, losses)\n","  total_val_losses = np.add(total_val_losses, val_losses)\n","  total_accs = np.add(total_accs, accs)\n","  total_val_accs = np.add(total_val_accs, val_accs)\n","\n","  best_i = np.argmin(val_losses)\n","  val_best_losses.append(val_losses[best_i])\n","  val_best_accuracies.append(val_accs[best_i])\n","\n","  ax1.plot(x, losses, c = \"bisque\", linewidth=2)\n","  ax2.plot(x, val_losses, c = \"bisque\", linewidth=2)\n","  ax3.plot(x, np.multiply(accs, 100), c = \"bisque\", linewidth=2)\n","  ax4.plot(x, np.multiply(val_accs, 100), c = \"bisque\", linewidth=2)\n","\n","mean_losses = total_losses / repeats\n","mean_val_losses = total_val_losses / repeats\n","mean_accs = total_accs / repeats\n","mean_val_accs = total_val_accs / repeats\n","\n","ax1.plot(x, mean_losses, c = \"darkorange\", linewidth=2)\n","ax2.plot(x, mean_val_losses, c = \"darkorange\", linewidth=2)\n","ax3.plot(x, np.multiply(mean_accs, 100), c = \"darkorange\", linewidth=2)\n","ax4.plot(x, np.multiply(mean_val_accs, 100), c = \"darkorange\", linewidth=2)\n","\n","ax1.set_xlabel(\"epoch\", fontsize=18)\n","ax1.set_ylabel(\"loss\", fontsize=18)\n","ax2.set_xlabel(\"epoch\", fontsize=18)\n","ax2.set_ylabel(\"loss\", fontsize=18)\n","ax3.set_xlabel(\"epoch\", fontsize=18)\n","ax3.set_ylabel(\"accuracy (%)\", fontsize=18)\n","ax4.set_xlabel(\"epoch\", fontsize=18)\n","ax4.set_ylabel(\"accuracy (%)\", fontsize=18)\n","\n","ax1.set_title('Training Loss', fontsize=18)\n","ax2.set_title('Validation Loss', fontsize=18)\n","ax3.set_title('Training Accuracy', fontsize=18)\n","ax4.set_title('Validation Accuracy', fontsize=18)\n","\n","for label in (ax1.get_xticklabels() + ax1.get_yticklabels()):\n","\tlabel.set_fontsize(14)\n"," \n","for label in (ax2.get_xticklabels() + ax2.get_yticklabels()):\n","\tlabel.set_fontsize(14)\n"," \n","for label in (ax3.get_xticklabels() + ax3.get_yticklabels()):\n","\tlabel.set_fontsize(14)\n"," \n","for label in (ax4.get_xticklabels() + ax4.get_yticklabels()):\n","\tlabel.set_fontsize(14)\n","\n","# plot val accuracies boxplot\n","fig2, ax = plt.subplots()\n","fig2.set_size_inches(5, 5)\n","ax.set_title(\"Validation Accuracy\", fontsize = 20)\n","\n","ax.boxplot(np.multiply(val_best_accuracies, 100))\n","ax.set_ylabel(\"Validation Accuracy (%)\", fontsize = 18)\n","ax.set_xticklabels([\"CNN\"], fontsize = 18)\n","\n","for label in ax.get_yticklabels():\n","\tlabel.set_fontsize(14)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"z76TMoa_vQ-t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Show test results"],"metadata":{"id":"-YLyHSxNundO"}},{"cell_type":"code","source":["best_model_i = np.argmin(val_best_losses)\n","model.load_weights(\"best_model_r{}\".format(best_model_i))\n","\n","\n","\n","\n","print(\"\\nTEST:\")\n","print(model.summary())\n","predictions = test(model, testX, testy, batch_size = batch_size, plot_features=True)\n","true_y = np.argmax(testy, axis = 1)\n","pred_y = np.argmax(predictions, axis = 1)\n","target_names = ['Walking', 'Walking Up', 'Walking Down', 'Sitting', 'Standing', 'Laying']\n","print(\"\\nClassification Report\")\n","print(classification_report(true_y, pred_y, target_names=target_names))"],"metadata":{"id":"umDiWduonDC3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Load tensorboard"],"metadata":{"id":"Xp71SHo9X-6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drAh2YjZLlFz"},"source":["%tensorboard --logdir logs/train"],"execution_count":null,"outputs":[]}]}