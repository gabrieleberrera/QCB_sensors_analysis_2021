{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tesing_models.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":[""],"metadata":{"id":"Xhy7kz-fWDp5"}},{"cell_type":"markdown","source":["# Impoprt Files and Dataset"],"metadata":{"id":"K1q7kLsahEdj"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t8WigDbUhNvX","executionInfo":{"status":"ok","timestamp":1645550060338,"user_tz":-60,"elapsed":2067,"user":{"displayName":"Gabriele Berrera","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09978087942630895371"}},"outputId":"7f964db9-3504-41e7-ea7e-495ae3d58b95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!unzip \"gdrive/My Drive/Tesi/PD_Detection/dataset.zip\" -d .\n","!mv dataset data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSe9esKzhOsf","executionInfo":{"status":"ok","timestamp":1645550107099,"user_tz":-60,"elapsed":46435,"user":{"displayName":"Gabriele Berrera","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09978087942630895371"}},"outputId":"61fe053d-459c-474d-b83e-acc15edfa9f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  gdrive/My Drive/Tesi/PD_Detection/dataset.zip\n","   creating: ./dataset/\n","  inflating: ./dataset/codes.npy     \n","  inflating: ./dataset/labels.npy    \n","  inflating: ./dataset/out_data.npy  \n","  inflating: ./dataset/rest_data.npy  \n","  inflating: ./dataset/rtn_data.npy  \n"]}]},{"cell_type":"code","source":["%cp gdrive/MyDrive/Tesi/PD_Detection/data_manager.py ."],"metadata":{"id":"LuhzbU6XhUQ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import Libreries\n","\n"],"metadata":{"id":"XULtHSpMfnN3"}},{"cell_type":"code","source":["import data_manager as dm\n","import tensorflow as tf\n","import math\n","import numpy as np\n","import random\n","import cv2\n","\n","import logging\n","from tensorflow import keras\n","from matplotlib import pyplot as plt\n","\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GroupShuffleSplit\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import roc_auc_score\n","\n","tf.get_logger().setLevel(logging.ERROR)"],"metadata":{"id":"tkcci68MfcKl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"QtjBCCKCJSgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Conv1D\n","from tensorflow.keras.layers import MaxPooling1D\n","from tensorflow.keras.layers import Dropout\n","import tensorflow.keras as keras\n"],"metadata":{"id":"5VXmAjAwfW3T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Original Network"],"metadata":{"id":"vo-ta2yCxCNa"}},{"cell_type":"code","source":["def get_original(input_shape):\n","    \n","    model = Sequential()\n","    model.add(Conv1D(filters=8, kernel_size=5, activation='relu', input_shape=input_shape))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=16, kernel_size=5, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=32, kernel_size=4, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=32, kernel_size=4, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Flatten())\n","    model.add(Dense(1, activation='sigmoid'))\n","     \n","    return model"],"metadata":{"id":"pN4h_Q1cxFLL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# With softmax"],"metadata":{"id":"D_IkCn7dxH7o"}},{"cell_type":"code","source":["def get_original_soft(input_shape):\n","    \n","    model = Sequential()\n","    model.add(Conv1D(filters=8, kernel_size=5, activation='relu', input_shape=input_shape))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=16, kernel_size=5, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=32, kernel_size=4, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=32, kernel_size=4, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Flatten())\n","    model.add(Dense(2, activation='softmax'))\n","     \n","    return model"],"metadata":{"id":"Y5gkpPqCxKZf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1D ResNet"],"metadata":{"id":"b6k5Lt98e_fG"}},{"cell_type":"code","source":["def conv_block(n_filters):\n","\n","    conv = Sequential()\n","\n","    conv.add(keras.layers.Conv1D(filters=n_filters, kernel_size=8, padding='same'))\n","    conv.add(keras.layers.BatchNormalization())\n","    conv.add(keras.layers.Activation('relu'))\n","\n","    conv.add(keras.layers.Conv1D(filters=n_filters, kernel_size=5, padding='same'))\n","    conv.add(keras.layers.BatchNormalization())\n","    conv.add(keras.layers.Activation('relu'))\n","\n","    conv.add(keras.layers.Conv1D(filters=n_filters, kernel_size=3, padding='same'))\n","    conv.add(keras.layers.BatchNormalization())\n","\n","    return conv"],"metadata":{"id":"TQbvDXvgRJJ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def shortcut(n_filters = None):\n","\n","    shortcut = Sequential()\n","\n","    # if n_filters is set, the shape of the input is changed according to it\n","    if n_filters:\n","      shortcut.add(keras.layers.Conv1D(filters=n_filters, kernel_size=1, padding='same'))\n","\n","    shortcut.add(keras.layers.BatchNormalization())\n","\n","    return shortcut"],"metadata":{"id":"5V8vaknSTkTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvtuZ1zdgUuc"},"outputs":[],"source":["def get_resnet(input_shape):\n","\n","    input_layer = keras.layers.Input(input_shape)\n","\n","    # Block 1\n","    conv_1 = conv_block(n_filters = 64)(input_layer)\n","    short_1 = shortcut(n_filters = 64)(input_layer)\n","\n","    output_1 = keras.layers.add([conv_1, short_1])\n","    output_1 = keras.layers.Activation('relu')(output_1)\n","\n","    # Block 2\n","    conv_2 = conv_block(n_filters = 128)(output_1)\n","    short_2 = shortcut(n_filters = 128)(output_1)\n","\n","    output_2 = keras.layers.add([conv_2, short_2])\n","    output_2 = keras.layers.Activation('relu')(output_2)\n","\n","    # Block 3\n","    conv_3 = conv_block(n_filters = 128)(output_2)\n","    short_3 = shortcut()(output_2)\n","\n","    output_3 = keras.layers.add([conv_3, short_3])\n","    output_3 = keras.layers.Activation('relu')(output_3)\n","\n","    # Final output\n","    final_output = keras.layers.GlobalAveragePooling1D(name=\"features\")(output_3)\n","    final_output = keras.layers.Dense(1, activation='sigmoid')(final_output)  \n","    model = keras.models.Model(inputs=input_layer, outputs=final_output)\n","    \n","    return model"]},{"cell_type":"markdown","source":["# HMM Classifier"],"metadata":{"id":"8YC48rMrCjzb"}},{"cell_type":"code","source":["class HMM_classifier():\n","\n","  def __init__(self, n_outputs, n_signals, n_states, n_gmodels):\n","\n","    self.n_outputs = n_outputs\n","    self.n_signals = n_signals\n","    self.n_states = n_states\n","    self.n_gmodels = n_gmodels\n","    self.models = []\n","\n","  \n","  def __format_input(self, X):\n","\n","    if len(X.shape) < 3:\n","      X = np.expand_dims(X, 2)\n","\n","    lens = np.repeat(X.shape[1], X.shape[0])\n","\n","    return np.concatenate(X), lens\n","\n","\n","  def fit(self, X, y):\n","\n","    predictions = []\n","\n","    for label in range(self.n_outputs):\n","\n","      inds = np.where(y == label)[0]\n","\n","      for i in range(self.n_signals):\n","        hmm = GMMHMM(self.n_states, self.n_gmodels)\n","\n","        data = X[i].copy()\n","        data, lens = self.__format_input(data[inds])\n","\n","        hmm.fit(data, lens)\n","\n","        self.models.append(hmm)\n","\n","\n","    print(\"ciao\")\n","    self.models = np.reshape(self.models, (self.n_signals,-1))\n","\n","\n","  def predict(self, X):\n","\n","    total_output = []\n","    \n","    for i, sig in enumerate(X):\n","      if len(sig.shape) < 3:\n","        sig = np.expand_dims(sig, 2)\n","\n","      for hmm in self.models[i]:\n","        preds = []\n","        for seq in sig:\n","          preds.append(hmm.score(seq))\n","        total_output.append(np.array(preds))\n","\n","    total_output = np.array(total_output)\n","\n","    print(total_output.shape)\n","\n","    return self.__compute_final_preds(total_output.T)\n","\n","  def __compute_final_preds(self, hmm_out):\n","    preds = []\n","    for row in hmm_out:\n","      row = np.reshape(row,(self.n_signals,-1))\n","      pred = np.argmax(np.sum(row, axis=0).flatten())\n","      preds.append(pred)\n","    return np.array(preds)\n","\n"],"metadata":{"id":"tMcmmZhuCjL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install hmmlearn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Yyzn2mlMtcy","executionInfo":{"status":"ok","timestamp":1645550115948,"user_tz":-60,"elapsed":3066,"user":{"displayName":"Gabriele Berrera","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09978087942630895371"}},"outputId":"5fc2afb5-1a31-4e2b-e71b-c908cd51c940"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: hmmlearn in /usr/local/lib/python3.7/dist-packages (0.2.7)\n","Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.4.1)\n","Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.21.5)\n","Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.0.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->hmmlearn) (3.1.0)\n"]}]},{"cell_type":"code","source":["from hmmlearn.hmm import GMMHMM\n"],"metadata":{"id":"_iZEMfkZL8Rr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Trasformations"],"metadata":{"id":"P0f3wC0hfZ3Y"}},{"cell_type":"code","source":["def norm_axis(a,b,c):\n","    newa=a/(math.sqrt(float(a*a+b*b+c*c)))\n","    newb=b/(math.sqrt(float(a*a+b*b+c*c)))\n","    newc=c/(math.sqrt(float(a*a+b*b+c*c)))\n","    \n","    return ([newa,newb,newc])\n","\n","\n","def rotation_matrix(axis, theta):\n","    axis = np.asarray(axis)\n","    axis = axis/math.sqrt(np.dot(axis, axis))\n","    \n","    a = math.cos(theta/2.0)\n","    b, c, d = -axis*math.sin(theta/2.0)\n","    \n","    aa, bb, cc, dd = a*a, b*b, c*c, d*d\n","    bc, ad, ac, ab, bd, cd = b*c, a*d, a*c, a*b, b*d, c*d\n","    \n","    return np.array([[aa+bb-cc-dd, 2*(bc+ad), 2*(bd-ac)], \n","                     [2*(bc-ad), aa+cc-bb-dd, 2*(cd+ab)], \n","                     [2*(bd+ac), 2*(cd-ab), aa+dd-bb-cc]])\n","\n","\n","def rand_rotate(image): ## theta: angle, a, b, c, eular vector\n","    theta = random.random()*math.pi*2\n","    a = random.random()\n","    b = random.random()\n","    c = random.random()\n","    \n","    axis=norm_axis(a,b,c)\n","    rot_matrix = rotation_matrix(axis, theta).T\n","    \n","    if image.shape[1] == 6:\n","        \n","        # Multipy the rotation matrix for both acceleration and rotation signals\n","        imagenew_acc = np.dot(image[:,:3], rot_matrix)\n","        imagenew_rot = np.dot(image[:,3:],rot_matrix)\n","        \n","        return np.concatenate([imagenew_acc, imagenew_rot], 1)\n","    \n","    elif image.shape[1] == 3:\n","        \n","        return np.dot(image, rot_matrix)\n","    \n","    else:\n","        \n","        # Implement for other types of data with different channels\n","        \n","        raise Exception(\"Cahnnels Error: rotateC works only on sequences with 3 or 6 channels\")\n","\n","\n","def rand_rescale_frequency(image,min_scale, max_scale):\n","    r = random.random()\n","    scale = r * (max_scale - min_scale) + min_scale\n","    \n","    [x,y]= image.shape\n","    y1=y\n","    x1=int(x*scale)\n","    image=cv2.resize(image,(y1,x1))\n","    new=np.zeros((x,y))\n","    if (x1>x):\n","        start=0\n","        end=start+x\n","        new=image[start:end]\n","    else:\n","        new_start=0\n","        new_end=new_start+x1\n","        new[new_start:new_end]=image\n","    return new"],"metadata":{"id":"a18drn_lft2m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Excecution"],"metadata":{"id":"bNUB3-5QgDjG"}},{"cell_type":"markdown","source":["Create logs direcory"],"metadata":{"id":"bESpRC873VZs"}},{"cell_type":"code","source":["%rm -r logs\n","%mkdir logs"],"metadata":{"id":"ayjbmTmI0nGZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Parameters"],"metadata":{"id":"B6Z0pGlViiuK"}},{"cell_type":"code","source":["batch_size = 64\n","epochs = 20\n","min_scale = 0.8\n","max_scale = 1.2\n","\n","size = 5000\n","activities = [\"out\", \"rtn\", \"rest\"]"],"metadata":{"id":"T2yqM4aIgQrz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNN vs ResNet"],"metadata":{"id":"woGS32f-ilff"}},{"cell_type":"code","source":["labels = dm.load_labels().astype(int)\n","codes = dm.load_codes()\n","\n","le = LabelEncoder()\n","codes = le.fit_transform(codes)\n","\n","model1_accs = []\n","model2_accs = []\n","for act in activities:\n","  data = dm.load_data(act, signal = \"acc\")\n","\n","  gss = GroupShuffleSplit(n_splits=1, train_size=0.5, random_state=1)\n","  for train_i, _ in gss.split(data, labels, groups=codes):\n","    train_i = train_i\n","\n","  data = data[train_i]\n","  train_labels = labels[train_i]\n","\n","  n_samples = data.shape[0]\n","  seq_len = data.shape[1]\n","  channels = data.shape[2]\n","\n","  train_set, val_set, train_labels, val_labels = train_test_split(data, train_labels, test_size=0.33, random_state=1)\n","\n","  train_set = np.array([rand_rotate(rand_rescale_frequency(x, min_scale, max_scale)) for x in train_set])\n","\n","  train_set = tf.data.Dataset.from_tensor_slices((train_set, train_labels))\n","  val_set = tf.data.Dataset.from_tensor_slices((val_set, val_labels))\n","\n","  train_set = train_set.batch(batch_size)\n","  val_set = val_set.batch(batch_size)\n","\n","  # Original CNN\n","  model1 = get_resnet((seq_len, channels))\n","\n","  check_point = tf.keras.callbacks.ModelCheckpoint(\"gdrive/My Drive/Tesi/PD_Detection/resnet/{}_cnn\".format(act), \n","                                                  monitor='val_loss', \n","                                                  save_weights_only=True, \n","                                                  save_best_only=True)\n","\n","  model1.compile(optimizer = tf.keras.optimizers.Adam(),\n","                loss = tf.keras.losses.BinaryCrossentropy(),\n","                metrics = ['AUC'])\n","\n","  history1 = model1.fit(train_set,\n","                        epochs = epochs, \n","                        batch_size = batch_size, \n","                        validation_data = val_set,\n","                        callbacks = [check_point],\n","                        verbose = 1)\n","  \n","  \n","  model1_accs.append(np.max(history1.history[\"val_auc\"]))\n","\n","  # ResNet\n","  model2 = get_resnet((seq_len, channels))\n","\n","  check_point2 = tf.keras.callbacks.ModelCheckpoint(\"gdrive/My Drive/Tesi/PD_Detection/resnet/{}_resnet\".format(act), \n","                                                  monitor='val_loss', \n","                                                  save_weights_only=True, \n","                                                  save_best_only=True)\n","\n","  model.compile(optimizer = tf.keras.optimizers.Adam(),\n","                loss = tf.keras.losses.BinaryCrossentropy(),\n","                metrics = ['AUC'])\n","\n","  history2 = model2.fit(train_set,\n","                        epochs = epochs, \n","                        batch_size = batch_size, \n","                        validation_data = val_set,\n","                        callbacks = [check_point],\n","                        verbose = 1)\n","  \n","  \n","  model1_accs.append(np.max(history1.history[\"val_auc\"]))\n","  model2_accs.append(np.max(history2.history[\"val_auc\"]))\n","  \n","\n","print(activities)\n","print(model1_accs)\n","print(model2_accs)"],"metadata":{"id":"tTNdEaP7kjoe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Improved ResNet Versions"],"metadata":{"id":"JSpeCnohlJCW"}},{"cell_type":"code","source":["from sklearn import svm\n","from sklearn.ensemble import RandomForestClassifier\n","\n","labels = dm.load_labels().astype(int)\n","codes = dm.load_codes()\n","\n","le = LabelEncoder()\n","codes = le.fit_transform(codes)\n","\n","total_val_features = []\n","total_test_features = []\n","total_test_predictions = []\n","\n","gss = GroupShuffleSplit(n_splits=1, train_size=0.5, random_state=1)\n","\n","for act in activities:\n","  data = dm.load_data(act, signal = \"acc\")\n","\n","  seq_len = data.shape[1]\n","  channels = data.shape[2]\n","\n","  for train_i, test_i in gss.split(data, labels, groups=codes):\n","    train_i = train_i\n","    test_i = test_i\n","\n","  train_set = data[train_i]\n","  train_labels = labels[train_i]\n","\n","  test_set = data[test_i]\n","  test_labels = labels[test_i]\n","\n","  train_set, val_set, train_labels, val_labels = train_test_split(train_set, train_labels, test_size=0.33, random_state=1)\n","\n","  train_set = np.array([rand_rotate(rand_rescale_frequency(x, min_scale, max_scale)) for x in train_set])\n","\n","  train_set = tf.data.Dataset.from_tensor_slices((train_set, train_labels))\n","  val_set = tf.data.Dataset.from_tensor_slices((val_set, val_labels))\n","\n","  train_set = train_set.batch(batch_size)\n","  val_set = val_set.batch(batch_size)\n","\n","  model = get_resnet((seq_len, channels))\n","\n","  model.load_weights(\"gdrive/My Drive/Tesi/PD_Detection/resnet/{}_resnet\".format(act))\n","\n","  extractor = tf.keras.models.Model(\n","    inputs=model.inputs,\n","    outputs=model.layers[-2].output,\n","  )  \n","\n","  val_features = extractor.predict(val_set, batch_size=batch_size)\n","  total_val_features.append(val_features)\n","\n","  test_features = extractor.predict(test_set, batch_size=batch_size)\n","  total_test_features.append(test_features)\n","\n","  test_predictions = model.predict(test_set, batch_size=batch_size)\n","  total_test_predictions.append(test_predictions)\n","  \n","for _, test_i in gss.split(data, labels, groups=codes):\n","  test_i = test_i\n","\n","hmm = HMM_classifier(2,len(activities),8,3)\n","hmm.fit(total_val_features, val_labels)\n","hmm_predictions = hmm.predict(total_test_features)\n","\n","total_val_features = np.concatenate(total_val_features, axis=1)\n","total_test_features = np.concatenate(total_test_features, axis=1)\n","\n","svm_classifier = svm.SVC()\n","svm_classifier.fit(total_val_features, val_labels)\n","\n","rf_classifier = RandomForestClassifier(random_state=1)\n","rf_classifier.fit(total_val_features, val_labels)\n","\n","svm_predictions = svm_classifier.predict(total_test_features)\n","rf_predictions = rf_classifier.predict(total_test_features)\n","\n","\n","total_test_predictions = np.concatenate(total_test_predictions).T\n","total_test_predictions = np.digitize(total_test_predictions, [0.5])\n","res_predictions = np.array([np.bincount(x).argmax() for x in total_test_predictions])"],"metadata":{"id":"90J-VSNA1Nb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save(\"gdrive/My Drive/Tesi/PD_Detection/resnet/hmm_preds\",hmm_predictions)\n","np.save(\"gdrive/My Drive/Tesi/PD_Detection/resnet/svm_preds\",svm_predictions)\n","np.save(\"gdrive/My Drive/Tesi/PD_Detection/resnet/rf_preds\",rf_predictions)\n","np.save(\"gdrive/My Drive/Tesi/PD_Detection/resnet/res_preds\",res_predictions)"],"metadata":{"id":"fiqNuxOpUlNj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Plot Results"],"metadata":{"id":"eZzn2-QBxY-u"}},{"cell_type":"markdown","source":["Print classification report for each model"],"metadata":{"id":"7-8aWF85lg74"}},{"cell_type":"code","source":["labels = dm.load_labels().astype(int)\n","codes = dm.load_codes()\n","\n","le = LabelEncoder()\n","codes = le.fit_transform(codes)\n","\n","total_val_features = []\n","total_test_features = []\n","\n","gss = GroupShuffleSplit(n_splits=1, train_size=0.5, random_state=1)\n","\n","for _, test_i in gss.split(data, labels, groups=codes):\n","  test_i = test_i\n","\n","y = labels[test_i]\n","\n","hmm_predictions = np.load(\"gdrive/My Drive/Tesi/PD_Detection/resnet/hmm_preds.npy\")\n","svm_predictions = np.load(\"gdrive/My Drive/Tesi/PD_Detection/resnet/svm_preds.npy\")\n","rf_predictions = np.load(\"gdrive/My Drive/Tesi/PD_Detection/resnet/rf_preds.npy\")\n","res_predictions = np.load(\"gdrive/My Drive/Tesi/PD_Detection/resnet/res_preds.npy\")\n","\n","\n","print(\"HMM Results\\n\")\n","print(classification_report(y, hmm_predictions, digits = 3))\n","print(\"\\n AUC:\", roc_auc_score(y,hmm_predictions))\n","\n","print(\"\\n SVM Results\\n\")\n","print(classification_report(y, svm_predictions, digits = 3))\n","print(\"\\n AUC:\", roc_auc_score(y,svm_predictions))\n","\n","print(\"\\n RF Results\\n\")\n","print(classification_report(y, rf_predictions, digits = 3))\n","print(\"\\n AUC:\", roc_auc_score(y,rf_predictions))\n","\n","print(\"\\n ResNet Results\\n\")\n","print(classification_report(y, res_predictions, digits = 3))\n","print(\"\\n AUC:\", roc_auc_score(y,res_predictions))"],"metadata":{"id":"lt0CafXFwecI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tensorboard"],"metadata":{"id":"Iv1Mz8D03eAu"}},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir logs"],"metadata":{"id":"8Ony_P0z3gHM"},"execution_count":null,"outputs":[]}]}